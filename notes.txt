code from: https://pythonprogramming.net/q-learning-reinforcement-learning-python-tutorial/

Entrenamiento modelo p6:
modelo malito
1: uso de GPU
2: aun así tardaba 2 tardes para 2mil simulaciones
# Modelo medio
2: Incremento valores de orden de magnitud de las recompensas asi como del numero de movimientos por episodio. 
Aumenté el orden de magnitud de las recompensas para que fuese beneficióso o perjudicial comer o chocarese y que no 
se quedase el modelo moviendose entre 2 casillas solo
3: elimine de los movimientos posibles  el que se pudiese qudar quieto y tambien si se chocaba con pareces recibía 
una penalización y se acababa la simulación (esto fue clave para reducir el tiempo de entrenamiento drasticamente)
4: SAlvar el modelo al hacer control+c para asi poder continuar con el entrenamiento en distinos movimientos y 
tambien poder reentrenar un modelo ya guardado.
5: aumento del numero de jugadas por batch
# modelo experto:
reentrenamiento del modelo medio con menos jugadas posibles para evitar que se quede parado o alternando entre las 
2 mismas casillas (de 200 jugadas como máximo a 40)

Según terminé mis exámenes, decidí ponerme a investigar un poco acerca de un par de técnicas de aprendizaje por refuerzo que nos fueron introducidas en la asignatura de aprendizaje automático. Estas técnicas eran Q-learning y Deep Q-learning. Al buscar, encontré un curso en internet que recomiendo (https://pythonprogramming.net/q-learning-reinforcement-learning-python-tutorial/), pero cuando estaba probando el código para Deep Q-learning, en el cual una IA tenía que moverse por un tablero hasta encontrar comida, evitando un bloque de lava, me di cuenta de que era demasiado lento, con 2,000 simulaciones en un día entero, pretendiendo llegar a 20,000, y sin funciones para guardar el modelo en mitad del entrenamiento para continuar en otro momento. Así que decidí realizar algunas mejoras. Lo primero que hice fue usar la GPU en lugar de la CPU, reduciendo bastante el tiempo de predicción y entrenamiento de la red. Además, eliminé la posibilidad de que el jugador se quedara parado, ya que si el objetivo era que fuera capaz de llegar al bloque de comida, no tenía mucho sentido. Esto redujo significativamente el tiempo de cada simulación realizada, pero aún no era suficiente. Así que decidí realizar más modificaciones:

1)Cambié el valor de las recompensas y penalizaciones para que fuera "rentable" ir a la zona de comida en lugar de quedarse dando pasos entre 2 bloques, o que no fuera buscando a lo loco, ya que caer en el bloque de lava suponía una penalización bastante grande.

2)Si se chocaba con los bordes del tablero, la simulación acababa y recibía una penalización también. Este fue uno de los mejores cambios que pude hacer, ya que se redujo muchísimo el tiempo de simulaciones y la IA era capaz de aprender muchísima más información en menos tiempo. Pasé de 2,000 simulaciones en un día entero a 2,000 simulaciones en apenas 2 horas.

3)Realicé una función que capturaba la señal de SIGINT para que pudiera guardar el modelo en cualquier momento al finalizar la simulación a mano con Control+C.

4)Creé un script en Python para visualizar el comportamiento de cada modelo que iba entrenando.

5)Añadí la posibilidad de reentrenar un modelo ya creado en lugar de que solo se pudiera crear un modelo y entrenarlo desde cero.

Con estas modificaciones, la IA resultante ya era bastante mejor, aunque no estaba del todo satisfecho, ya que tardaba demasiado en ir a por el bloque de comida o simplemente se movia entre 2 bloques esperando a llegar al número de pasos máximos de la simulación. Así que decidí reentrenar el último modelo que tenía, pero reduciendo el coeficiente de exploración (para que no buscara al azar todo el tiempo, ya que no estaba entrenando el modelo desde cero) y reduje el número máximo de pasos posibles de la IA por simulación de 200 a 40 (había una penalización pequeña por cada paso que se daba). Y voilà, obtuve por fin la IA que buscaba.

Finalmente, al ver que la reducción de pasos máximos de la IA por simulación mejoró bastante el modelo anterior, reentrené esta IA ya "experta" en la tarea a realizar. Volví a reducir este número de pasos de 40 a 20, obteniendo ya una IA robusta para realizar la tarea en cuestión.

Conclusiones: esta última parte del curso me pareció bastante interesante y me gustó que fuera capaz de obtener la IA que buscaba gracias a los conocimientos aprendidos en él. También quiero decir que, pese a que el modelo no es perfecto y se podría seguir entrenando, no es mi objetivo obtener un modelo perfecto, sino entender cómo funcionan estas técnicas, dando por concluido este mini-experimento de 3 días. También considerar la posibilidad de que este tipo de cosas que parecen de juguete realmente pueden ser utilizadas para problemas reales. Por ejemplo, si en lugar de la imagen de un tablero de 10x10 le paso la imagen de un mapa de una habitación y me voy guardando cada paso que da la IA hasta llegar al "destino/bloque de comida", ¿no podría tener el principio de un planificador de rutas basado en aprendizaje por refuerzo? Obviamente, habría que trabajar bastante más el modelo para eso, pero la posibilidad está ahí. Espero que os haya resultado interesante al menos. Os dejo un enlace a una demo visual de los 4 modelos que obtuve en cada fase de mejora explicada anteriormente.
